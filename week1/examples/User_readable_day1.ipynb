{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: PDF Summarizer - Day 1\n",
    "## Complete Pipeline with LLM Integration\n",
    "\n",
    "**Location**: `week1/examples/day1.ipynb`\n",
    "\n",
    "This notebook demonstrates an end-to-end PDF summarization pipeline using:\n",
    "- **PDF Parsing**: Extract text from PDF files\n",
    "- **Token-Based Chunking**: Respect LLM context window limits\n",
    "- **Multi-Stage Summarization**: Chunk-level + final synthesis\n",
    "- **LLM Integration**: Local models via Ollama (TinyLLama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Dependencies\n",
    "Import all required libraries and modules from pdf_summarizer package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/nexonsamuel/Documents/data_engg_tutor/week1/examples\n",
      "Adding to path: /Users/nexonsamuel/Documents/data_engg_tutor/week1\n",
      "Path contents: ['.DS_Store', 'requirements.txt', 'tests', 'output', 'ollama.sh', 'README.md', 'examples', 'pdf_summarizer', 'data']\n",
      "✓ All dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# For Jupyter notebooks - get parent directory\n",
    "current_dir = Path.cwd()\n",
    "week1_path = str(current_dir.parent)  # Go up one level from examples/\n",
    "sys.path.insert(0, week1_path)\n",
    "\n",
    "# Verify\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Adding to path: {week1_path}\")\n",
    "print(f\"Path contents: {os.listdir(week1_path)}\")\n",
    "\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "from pdf_summarizer.pdf_parser import pdf_parser_func\n",
    "from pdf_summarizer.model_constants import MODEL_CONFIGS, DEFAULT_MODEL, DEFAULT_SAFETY_FACTOR\n",
    "\n",
    "print(\"✓ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Configuration & Constants\n",
    "Centralized settings for LLM, chunking, and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== LLM Configuration ====================\n",
    "# Using configurations from pdf_summarizer.model_constants\n",
    "\n",
    "LLM_CONFIG = MODEL_CONFIGS.get(DEFAULT_MODEL, MODEL_CONFIGS['tinyllama'])\n",
    "\n",
    "# ==================== Chunking Configuration ====================\n",
    "# Token-based chunking parameters\n",
    "\n",
    "CHUNKING_CONFIG = {\n",
    "    'max_tokens': 500,                    # Maximum tokens per chunk\n",
    "    'encoding': LLM_CONFIG.get('encoding', 'cl100k_base')  # Use model encoding\n",
    "}\n",
    "\n",
    "# ==================== Prompt Templates ====================\n",
    "# Define system prompts for different stages of processing\n",
    "\n",
    "PROMPTS = {\n",
    "    'chunk_summarizer': (\n",
    "        'You are evaluating a Data Engineer candidate. '\n",
    "        'Extract only: work experience, technical skills, cloud platforms, and key achievements. '\n",
    "        'Be factual and concise.'\n",
    "    ),\n",
    "    'final_synthesizer': (\n",
    "        'You are an AI Head of a data engineering team evaluating a candidate\\'s resume '\n",
    "        'for a Data Engineer position. Your job is to provide a crisp, professional evaluation '\n",
    "        'based on the information provided. Assess their: 1) Relevant experience, 2) Technical skills, '\n",
    "        '3) Cloud platform expertise, 4) Data pipeline/ETL knowledge, 5) Overall fit for the role. '\n",
    "        'Be objective and constructive.'\n",
    "    )\n",
    "}\n",
    "\n",
    "# ==================== Output Configuration ====================\n",
    "OUTPUT_DIR = Path('../output')  # Save to week1/output/\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  LLM Model: {LLM_CONFIG['name']}\")\n",
    "print(f\"  Provider: {LLM_CONFIG['provider']}\")\n",
    "print(f\"  Max Tokens per Chunk: {CHUNKING_CONFIG['max_tokens']}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Utility Functions\n",
    "Helper functions for text processing and LLM interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_by_tokens(text, max_tokens=500, encoding='cl100k_base'):\n",
    "    \"\"\"\n",
    "    Split text into chunks based on token count.\n",
    "    \n",
    "    This function respects LLM context window limits by splitting large documents\n",
    "    into smaller chunks that can be processed independently.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        max_tokens (int): Maximum tokens per chunk (default: 500)\n",
    "        encoding (str): Tiktoken encoding name (default: 'cl100k_base')\n",
    "    \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    encoder = tiktoken.get_encoding(encoding)\n",
    "    \n",
    "    # Tokenize the entire text\n",
    "    tokens = encoder.encode(text)\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    print(f\"Total tokens in document: {total_tokens}\")\n",
    "    print(f\"Max tokens per chunk: {max_tokens}\")\n",
    "    \n",
    "    # Split tokens into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = encoder.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def initialize_llm_client(config=None):\n",
    "    \"\"\"\n",
    "    Initialize OpenAI client configured for local Ollama server.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): LLM configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        OpenAI: Configured OpenAI client instance\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = LLM_CONFIG\n",
    "    \n",
    "    client = OpenAI(\n",
    "        base_url=config['base_url'],\n",
    "        api_key=config['api_key']\n",
    "    )\n",
    "    print(f\"LLM client initialized (Model: {config['name']})\")\n",
    "    return client\n",
    "\n",
    "\n",
    "def call_llm(client, messages, model_name=None):\n",
    "    \"\"\"\n",
    "    Send messages to LLM and get response.\n",
    "    \n",
    "    Args:\n",
    "        client (OpenAI): Initialized OpenAI client\n",
    "        messages (list): List of message dicts with 'role' and 'content'\n",
    "        model_name (str): Model name to use\n",
    "    \n",
    "    Returns:\n",
    "        str: Response text from LLM\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = LLM_CONFIG['name']\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(\"✓ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Pipeline Functions\n",
    "Core functions that orchestrate the summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_path, max_tokens=None):\n",
    "    \"\"\"\n",
    "    Complete PDF summarization pipeline:\n",
    "    1. Parse PDF and extract text\n",
    "    2. Chunk text by tokens\n",
    "    3. Summarize each chunk independently\n",
    "    4. Combine chunk summaries\n",
    "    5. Create final synthesis\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF file\n",
    "        max_tokens (int): Maximum tokens per chunk\n",
    "    \n",
    "    Returns:\n",
    "        dict: Result dictionary containing final_summary, chunk_summaries, etc.\n",
    "    \"\"\"\n",
    "    if max_tokens is None:\n",
    "        max_tokens = CHUNKING_CONFIG['max_tokens']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING PDF SUMMARIZATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ==================== Step 1: Initialize Client ====================\n",
    "    print(\"\\n[STEP 1] Initializing LLM client...\")\n",
    "    client = initialize_llm_client()\n",
    "    \n",
    "    # ==================== Step 2: Parse PDF ====================\n",
    "    print(\"\\n[STEP 2] Parsing PDF...\")\n",
    "    try:\n",
    "        pdf_text = pdf_parser_func(pdf_path)\n",
    "        print(f\"✓ PDF parsed successfully\")\n",
    "        print(f\"  Document size: {len(pdf_text)} characters\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse PDF: {str(e)}\")\n",
    "    \n",
    "    # ==================== Step 3: Chunk Text ====================\n",
    "    print(\"\\n[STEP 3] Chunking text by tokens...\")\n",
    "    chunks = chunk_text_by_tokens(\n",
    "        text=pdf_text,\n",
    "        max_tokens=max_tokens,\n",
    "        encoding=CHUNKING_CONFIG['encoding']\n",
    "    )\n",
    "    \n",
    "    # ==================== Step 4: Summarize Each Chunk ====================\n",
    "    print(\"\\n[STEP 4] Summarizing each chunk...\")\n",
    "    chunk_summaries = []\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Processing chunk {idx}/{len(chunks)}...\", end=\" \")\n",
    "        \n",
    "        # Prepare messages for this chunk\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': PROMPTS['chunk_summarizer']\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"Summarize this text:\\n\\n{chunk}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Call LLM and collect summary\n",
    "        try:\n",
    "            summary = call_llm(client, messages)\n",
    "            chunk_summaries.append(summary)\n",
    "            print(\"✓\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {str(e)}\")\n",
    "            chunk_summaries.append(f\"[Error processing chunk {idx}]\")\n",
    "    \n",
    "    print(f\"✓ All {len(chunks)} chunks summarized\")\n",
    "    \n",
    "    # ==================== Step 5: Combine Summaries ====================\n",
    "    print(\"\\n[STEP 5] Combining chunk summaries...\")\n",
    "    combined_summary = \"\\n\\n\".join([\n",
    "        f\"[CHUNK {idx}]\\n{summary}\"\n",
    "        for idx, summary in enumerate(chunk_summaries, 1)\n",
    "    ])\n",
    "    print(\"✓ Summaries combined\")\n",
    "    \n",
    "    # ==================== Step 6: Create Final Summary ====================\n",
    "    print(\"\\n[STEP 6] Creating final synthesis...\")\n",
    "    final_messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': PROMPTS['final_synthesizer']\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f\"Based on this candidate's information, provide a concise hiring evaluation for a Data Engineer role:\\n\\n{combined_summary}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        final_summary = call_llm(client, final_messages)\n",
    "        print(\"✓ Final summary created\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error creating final summary: {str(e)}\")\n",
    "        final_summary = combined_summary  # Fallback\n",
    "    \n",
    "    # ==================== Step 7: Prepare Results ====================\n",
    "    print(\"\\n[STEP 7] Preparing results...\")\n",
    "    results = {\n",
    "        'final_summary': final_summary,\n",
    "        'chunk_summaries': chunk_summaries,\n",
    "        'num_chunks': len(chunks),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'pdf_path': pdf_path,\n",
    "        'model': LLM_CONFIG['name']\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_results(results, output_filename=None):\n",
    "    \"\"\"\n",
    "    Save summarization results to file with metadata.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results dictionary from summarize_pdf()\n",
    "        output_filename (str): Optional custom filename (without extension)\n",
    "    \n",
    "    Returns:\n",
    "        Path: Path to saved file\n",
    "    \"\"\"\n",
    "    # Generate filename if not provided\n",
    "    if output_filename is None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_filename = f\"summary_{timestamp}.txt\"\n",
    "    else:\n",
    "        output_filename = f\"{output_filename}.txt\"\n",
    "    \n",
    "    output_path = OUTPUT_DIR / output_filename\n",
    "    \n",
    "    # Prepare output content with metadata\n",
    "    output_content = f\"\"\"\n",
    "{'='*80}\n",
    "PDF SUMMARIZATION RESULT\n",
    "{'='*80}\n",
    "\n",
    "Generated: {results['timestamp']}\n",
    "Model: {results['model']}\n",
    "PDF: {results['pdf_path']}\n",
    "Chunks Processed: {results['num_chunks']}\n",
    "\n",
    "{'='*80}\n",
    "FINAL SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "{results['final_summary']}\n",
    "\n",
    "\n",
    "{'='*80}\n",
    "CHUNK SUMMARIES\n",
    "{'='*80}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add chunk summaries\n",
    "    for idx, chunk_summary in enumerate(results['chunk_summaries'], 1):\n",
    "        output_content += f\"\\n[CHUNK {idx}]\\n{chunk_summary}\\n\\n\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(output_content)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "print(\"✓ Pipeline functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Test with Sample Data\n",
    "Run the complete pipeline on your resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Execute Pipeline ====================\n",
    "# Process your resume PDF\n",
    "\n",
    "# Use relative path from week1/examples/ to week1/data/\n",
    "PDF_FILE = '../data/Nexon_Samuel.pdf'\n",
    "\n",
    "# Verify file exists\n",
    "pdf_path = Path(PDF_FILE)\n",
    "if not pdf_path.exists():\n",
    "    print(f\"⚠️  PDF file not found at {pdf_path.absolute()}\")\n",
    "    print(f\"Place your PDF in: week1/data/\")\n",
    "else:\n",
    "    # Run the complete pipeline\n",
    "    results = summarize_pdf(PDF_FILE)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(results['final_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Save & Display Output\n",
    "Persist results and show formatted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results (only if results exist)\n",
    "if 'results' in locals():\n",
    "    output_path = save_results(results, output_filename='nexon_samuel_summary')\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXECUTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"PDF File: {results['pdf_path']}\")\n",
    "    print(f\"Chunks Processed: {results['num_chunks']}\")\n",
    "    print(f\"Model Used: {results['model']}\")\n",
    "    print(f\"Timestamp: {results['timestamp']}\")\n",
    "    print(f\"Output File: {output_path}\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"No results to save - run Section 5 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Format Output Display\n",
    "Display summary in formatted paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary formatted by paragraphs\n",
    "if 'results' in locals():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FORMATTED SUMMARY OUTPUT\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Split by periods to show paragraphs\n",
    "    summary_text = results['final_summary']\n",
    "    paragraphs = summary_text.split('. ')\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # Clean up and print each paragraph\n",
    "        para = para.strip()\n",
    "        if para:\n",
    "            print(f\"{para}.\")\n",
    "            print()  # Add blank line between paragraphs\n",
    "else:\n",
    "    print(\"No results to display - run Section 5 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Advanced Options\n",
    "Optional functions for batch processing and custom configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_pdfs(pdf_directory):\n",
    "    \"\"\"\n",
    "    Process multiple PDF files in a directory.\n",
    "    \n",
    "    Args:\n",
    "        pdf_directory (str): Path to directory containing PDF files\n",
    "    \n",
    "    Returns:\n",
    "        list: List of result dictionaries\n",
    "    \n",
    "    Example:\n",
    "        results = process_multiple_pdfs('../data/')\n",
    "    \"\"\"\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    pdf_files = list(pdf_dir.glob('*.pdf'))\n",
    "    \n",
    "    print(f\"\\nFound {len(pdf_files)} PDF files in {pdf_directory}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for idx, pdf_file in enumerate(pdf_files, 1):\n",
    "        print(f\"\\n[{idx}/{len(pdf_files)}] Processing: {pdf_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            results = summarize_pdf(str(pdf_file))\n",
    "            \n",
    "            # Save results\n",
    "            output_name = pdf_file.stem + '_summary'\n",
    "            save_results(results, output_filename=output_name)\n",
    "            \n",
    "            all_results.append(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {pdf_file.name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n✓ Processed {len(all_results)} PDFs successfully\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "print(\"✓ Advanced functions defined\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  # Process all PDFs in week1/data/ folder\")\n",
    "print(\"  all_results = process_multiple_pdfs('../data/')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pipeline Workflow\n",
    "```\n",
    "PDF File (week1/data/)\n",
    "   ↓\n",
    "Parse PDF → Extract Text\n",
    "   ↓\n",
    "Chunk by Tokens → Respect context window\n",
    "   ↓\n",
    "Summarize Each Chunk → LLM processing\n",
    "   ↓\n",
    "Combine Summaries → Merge results\n",
    "   ↓\n",
    "Final Synthesis → Create comprehensive summary\n",
    "   ↓\n",
    "Save Output → Generate timestamped file (week1/output/)\n",
    "```\n",
    "\n",
    "### Project Structure\n",
    "```\n",
    "week1/\n",
    "├── examples/\n",
    "│   └── day1.ipynb              # This notebook\n",
    "├── pdf_summarizer/\n",
    "│   ├── __init__.py\n",
    "│   ├── pdf_parser.py           # ← Imported\n",
    "│   ├── chunker.py\n",
    "│   ├── model_constants.py      # ← Imported\n",
    "│   └── summarizer.py\n",
    "├── data/                       # Input PDFs\n",
    "└── output/                     # Generated summaries\n",
    "```\n",
    "\n",
    "### How to Use\n",
    "1. **Place PDFs** in `week1/data/` folder\n",
    "2. **Section 5**: Change PDF_FILE path or use batch processing\n",
    "3. **Run sections 1-7** to execute pipeline\n",
    "4. **Check results** in `week1/output/` folder\n",
    "\n",
    "### Customization\n",
    "- **Change model** in Section 2: `LLM_CONFIG = MODEL_CONFIGS['mistral']`\n",
    "- **Adjust chunks** in Section 2: `'max_tokens': 1000`\n",
    "- **Modify prompts** in Section 2: `PROMPTS['chunk_summarizer'] = \"....\"`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
