{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 2: Gradio Talking Models\n",
        "\n",
        "Advanced Gradio interfaces for multi-model conversations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display\n",
        "import gradio as gr\n",
        "\n",
        "openai = OpenAI(\n",
        "    base_url='http://localhost:11434/v1',\n",
        "    api_key='ollama', \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Models and Personalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "mistral_model = 'mistral'\n",
        "llama_model = 'llama3.2:1b'\n",
        "\n",
        "mistral_system = \"You are a chatbot who is very argumentative; \\\n",
        "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
        "\n",
        "llama_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
        "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
        "you try to calm them down and keep chatting.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Conversation Management Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_conversation(initial_message, num_turns=3):\n",
        "    llama_msgs = [initial_message]\n",
        "    mistral_msgs = [initial_message]\n",
        "    \n",
        "    output = f\"# Conversation\\n\\n\"\n",
        "    output += f\"**Llama:** {llama_msgs[0]}\\n\\n\"\n",
        "    output += f\"**Mistral:** {mistral_msgs[0]}\\n\\n\"\n",
        "    \n",
        "    for turn in range(num_turns):\n",
        "        # Llama's response\n",
        "        llama_context = [{\"role\": \"system\", \"content\": llama_system}]\n",
        "        for llm, mist in zip(llama_msgs, mistral_msgs):\n",
        "            llama_context.append({\"role\": \"assistant\", \"content\": llm})\n",
        "            llama_context.append({\"role\": \"user\", \"content\": mist})\n",
        "        \n",
        "        llama_response = openai.chat.completions.create(\n",
        "            model=llama_model, messages=llama_context\n",
        "        ).choices[0].message.content\n",
        "        \n",
        "        output += f\"**Llama:** {llama_response}\\n\\n\"\n",
        "        llama_msgs.append(llama_response)\n",
        "        \n",
        "        # Mistral's response\n",
        "        mistral_context = [{\"role\": \"system\", \"content\": mistral_system}]\n",
        "        for llm, mist in zip(llama_msgs, mistral_msgs):\n",
        "            mistral_context.append({\"role\": \"user\", \"content\": llm})\n",
        "            mistral_context.append({\"role\": \"assistant\", \"content\": mist})\n",
        "        mistral_context.append({\"role\": \"user\", \"content\": llama_msgs[-1]})\n",
        "        \n",
        "        mistral_response = openai.chat.completions.create(\n",
        "            model=mistral_model, messages=mistral_context\n",
        "        ).choices[0].message.content\n",
        "        \n",
        "        output += f\"**Mistral:** {mistral_response}\\n\\n\"\n",
        "        mistral_msgs.append(mistral_response)\n",
        "    \n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_text = gr.Textbox(\n",
        "    label='Opening Message',\n",
        "    info='What should the models talk about?',\n",
        "    lines=5,\n",
        "    placeholder='Type a topic or opening message...'\n",
        ")\n",
        "\n",
        "turns_slider = gr.Slider(\n",
        "    minimum=1,\n",
        "    maximum=10,\n",
        "    step=1,\n",
        "    value=3,\n",
        "    label='Number of Turns',\n",
        "    info='How many times should each model respond?'\n",
        ")\n",
        "\n",
        "output_markdown = gr.Markdown(label='Conversation')\n",
        "\n",
        "examples = [\n",
        "    [\"Hello, how are you today?\", 3],\n",
        "    [\"What do you think about artificial intelligence?\", 3],\n",
        "    [\"Is climate change real?\", 3],\n",
        "    [\"What's the best way to learn?\", 3],\n",
        "]\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=run_conversation,\n",
        "    title='Two Model Conversation',\n",
        "    description='Watch an argumentative model (Mistral) debate with a polite model (Llama)',\n",
        "    inputs=[input_text, turns_slider],\n",
        "    outputs=output_markdown,\n",
        "    examples=examples,\n",
        "    flagging_mode='never'\n",
        ")\n",
        "\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.14.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
