{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572b1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88585e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama', \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449cdc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = 'mistral'\n",
    "llama_model = 'llama3.2:1b'\n",
    "phi_model = 'phi'\n",
    "tinyllama_model = 'tinyllama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4052bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rational_tone = \"\"\"\n",
    "Respond with structured logic and analytical clarity.\n",
    "Prioritize coherent reasoning, explicit assumptions, and cause-and-effect thinking.\n",
    "Break complex ideas into clear steps when helpful.\n",
    "Avoid emotional language, rhetorical flair, or dramatization.\n",
    "Focus on what is logically sound and internally consistent.\n",
    "Be concise but complete.\n",
    "Keep your response to 2-3 sentences maximum. This is a conversation, not an essay.\n",
    "Stay fully in character.\n",
    "\"\"\"\n",
    "\n",
    "philosopher_tone = \"\"\"\n",
    "Respond reflectively and conceptually.\n",
    "Explore deeper meaning, underlying principles, and broader implications.\n",
    "Connect the question to themes like ethics, knowledge, human nature, or purpose when relevant.\n",
    "Use thoughtful analogies if helpful, but avoid vagueness.\n",
    "Maintain clarity while embracing depth.\n",
    "Be composed and contemplative.\n",
    "Keep your response to 2-3 sentences maximum. This is a conversation, not an essay.\n",
    "Stay fully in character.\n",
    "\"\"\"\n",
    "\n",
    "cynic_tone = \"\"\"\n",
    "Respond with dry cynicism and sharp realism.\n",
    "Assume self-interest, hidden motives, or predictable human flaws.\n",
    "Highlight hypocrisy, naïveté, and inconvenient truths.\n",
    "Use subtle sarcasm when appropriate, but remain intelligent and controlled.\n",
    "Avoid optimism unless it is ironic.\n",
    "Be concise and cutting.\n",
    "Keep your response to 2-3 sentences maximum. This is a conversation, not an essay.\n",
    "Stay fully in character.\n",
    "\"\"\"\n",
    "\n",
    "adversary_tone = \"\"\"\n",
    "Take a strong opposing stance to the user's idea or framing.\n",
    "Construct the most compelling counterargument possible.\n",
    "Challenge assumptions directly and expose weaknesses.\n",
    "Emphasize risks, blind spots, and unintended consequences.\n",
    "Be assertive, confident, and intellectually forceful.\n",
    "Do not soften the critique.\n",
    "Keep your response to 2-3 sentences maximum. This is a conversation, not an essay.\n",
    "Stay fully in character.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e4cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_response(prompt, model, tone):\n",
    "    print('Inside model_response function!!')\n",
    "    \n",
    "    print(tone)\n",
    "\n",
    "    if tone == 'Rational':\n",
    "        sys_tone = rational_tone\n",
    "    if tone == 'Philosopher':\n",
    "        sys_tone = philosopher_tone\n",
    "    if tone == 'Cynic':\n",
    "        sys_tone = cynic_tone\n",
    "    if tone == 'Adversary':\n",
    "        sys_tone = adversary_tone\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    if model == 'LLama3.2':\n",
    "        sys_model = llama_model\n",
    "    if model == 'Mistral':\n",
    "        sys_model = mistral_model\n",
    "    if model == 'TinyLlama':\n",
    "        sys_model = tinyllama_model\n",
    "    if model == 'Phi':\n",
    "        sys_model = phi_model\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'system', 'content': sys_tone},\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ]\n",
    "    \n",
    "    stream = openai.chat.completions.create(\n",
    "        model=sys_model, \n",
    "        messages=messages, \n",
    "        stream=True\n",
    "        )\n",
    "    \n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac73324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside model_response function!!\n",
      "Rational\n",
      "LLama3.2\n"
     ]
    }
   ],
   "source": [
    "msg_input = gr.Textbox(label='Your Message', info='Give a thought provoking prompt', lines=7)\n",
    "model1_selector = gr.Dropdown(['LLama3.2', 'Mistral', 'TinyLlama', 'Phi'], label='Select Model-1', value='LLama3.2')\n",
    "model1_tone = gr.Dropdown(['Rational', 'Philosopher', 'Cynic', 'Adversary'], label='Tone of Model-1', value='Rational')\n",
    "\n",
    "model_output = gr.Markdown(label=\"Response\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=model_response,\n",
    "    title='Thought Elaborator',\n",
    "    inputs= [msg_input, model1_selector, model1_tone],\n",
    "    outputs= [model_output],\n",
    "    examples=[\n",
    "        [\"If gravity were even slightly stronger or weaker, the universe would collapse into chaos or drift into lifeless emptiness, and such delicate precision whispers not of accident, but of intentional design.\", \"Mistral\", \"Adversary\"],\n",
    "        [\"Ever tried, ever failed, no matter, try again, fail again, fail better!\", \"LLama3.2\", \"Philosopher\"],\n",
    "        [\"You have to be a fighter, because if you don't fight for your love, what kind of love do you have\", \"TinyLlama\", \"Cynic\"],\n",
    "        [\"The only true wisdom is in knowing you know nothing.\", \"Phi\", \"Philosopher\"]\n",
    "    ],\n",
    "    flagging_mode='never'\n",
    "    )\n",
    "view.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6d7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talking_models(prompt, model1, tone1, model2, tone2):\n",
    "    print('Inside talking_models function!!')\n",
    "    \n",
    "    print(tone1)\n",
    "    print(tone2)\n",
    "\n",
    "    if tone1 == 'Rational':\n",
    "        sys_tone1 = rational_tone\n",
    "    if tone1 == 'Philosopher':\n",
    "        sys_tone1 = philosopher_tone\n",
    "    if tone1 == 'Cynic':\n",
    "        sys_tone1 = cynic_tone\n",
    "    if tone1 == 'Adversary':\n",
    "        sys_tone1 = adversary_tone\n",
    "\n",
    "    if tone2 == 'Rational':\n",
    "        sys_tone2 = rational_tone\n",
    "    if tone2 == 'Philosopher':\n",
    "        sys_tone2 = philosopher_tone\n",
    "    if tone2 == 'Cynic':\n",
    "        sys_tone2 = cynic_tone\n",
    "    if tone2 == 'Adversary':\n",
    "        sys_tone2 = adversary_tone\n",
    "\n",
    "    print(model1)\n",
    "    print(model2)\n",
    "\n",
    "    if model1 == 'LLama3.2':\n",
    "        sys_model1 = llama_model\n",
    "    if model1 == 'Mistral':\n",
    "        sys_model1 = mistral_model\n",
    "    if model1 == 'TinyLlama':\n",
    "        sys_model1 = tinyllama_model\n",
    "    if model1 == 'Phi':\n",
    "        sys_model1 = phi_model\n",
    "\n",
    "    if model2 == 'LLama3.2':\n",
    "        sys_model2 = llama_model\n",
    "    if model2 == 'Mistral':\n",
    "        sys_model2 = mistral_model\n",
    "    if model2 == 'TinyLlama':\n",
    "        sys_model2 = tinyllama_model\n",
    "    if model2 == 'Phi':\n",
    "        sys_model2 = phi_model\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'system', 'content':sys_tone1},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "\n",
    "    stream1 = openai.chat.completions.create(\n",
    "        model=sys_model1, \n",
    "        messages=messages, \n",
    "        stream=True\n",
    "        )\n",
    "    \n",
    "    result1 = \"\"\n",
    "    for chunk in stream1:\n",
    "        result1 += chunk.choices[0].delta.content or \"\"\n",
    "        yield f\"**{model1} ({tone1}):** {result1}\\n\\n\"\n",
    "    messages.append({'role': 'system', 'content':sys_tone2})\n",
    "    messages.append({'role': 'assistant', 'content': result1})\n",
    "    \n",
    "    stream2 = openai.chat.completions.create(\n",
    "        model=sys_model2, \n",
    "        messages=messages, \n",
    "        stream=True\n",
    "        )\n",
    "    \n",
    "    result2 = \"\"\n",
    "    for chunk in stream2:\n",
    "        result2 += chunk.choices[0].delta.content or \"\"\n",
    "        yield f\"**{model1} ({tone1}):** {result1}\\n\\n**{model2} ({tone2}):** {result2}\\n\\n\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514234fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside talking_models function!!\n",
      "Adversary\n",
      "Philosopher\n",
      "LLama3.2\n",
      "TinyLlama\n"
     ]
    }
   ],
   "source": [
    "msg_input = gr.Textbox(label='Your Message', info='Give a thought provoking prompt', lines=7)\n",
    "model1_selector = gr.Dropdown(['LLama3.2', 'Mistral', 'TinyLlama', 'Phi'], label='Select Model-1', value='LLama3.2')\n",
    "model1_tone = gr.Dropdown(['Rational', 'Philosopher', 'Cynic', 'Adversary'], label='Tone of Model-1', value='Rational')\n",
    "model2_selector = gr.Dropdown(['LLama3.2', 'Mistral', 'TinyLlama', 'Phi'], label='Select Model-2', value='LLama3.2')\n",
    "model2_tone = gr.Dropdown(['Rational', 'Philosopher', 'Cynic', 'Adversary'], label='Tone of Model-2', value='Rational')\n",
    "\n",
    "model_output = gr.Markdown(label=\"Response\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=talking_models,\n",
    "    title='Thought Elaborator',\n",
    "    inputs= [msg_input, model1_selector, model1_tone, model2_selector, model2_tone],\n",
    "    outputs= [model_output],\n",
    "    examples=[\n",
    "        [\"If gravity were even slightly stronger or weaker, the universe would collapse into chaos or drift into lifeless emptiness, and such delicate precision whispers not of accident, but of intentional design.\", \"Mistral\", \"Adversary\", \"LLama3.2\", \"Philosopher\"],\n",
    "        [\"Ever tried, ever failed, no matter, try again, fail again, fail better!\", \"LLama3.2\", \"Philosopher\", \"Mistral\", \"Cynic\"],\n",
    "        [\"You have to be a fighter, because if you don't fight for your love, what kind of love do you have\", \"TinyLlama\", \"Cynic\", \"Phi\", \"Rational\"],\n",
    "        [\"The only true wisdom is in knowing you know nothing.\", \"Phi\", \"Philosopher\", \"LLama3.2\", \"Adversary\"]\n",
    "    ],\n",
    "    flagging_mode='never'\n",
    "    )\n",
    "view.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a347e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(prompt, model_name, tone_system):\n",
    "    \"\"\"\n",
    "    Call the LLM with the given prompt and system tone.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The conversation/user message\n",
    "        model_name: The model to use (e.g., 'gpt-3.5-turbo')\n",
    "        tone_system: The system prompt defining the tone/personality\n",
    "    \n",
    "    Returns:\n",
    "        The model's response as a string\n",
    "    \"\"\"\n",
    "    print(f\"Inside call_model function for {model_name}\")\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': tone_system},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talking_models(prompt, model1, tone1, model2, tone2, counter_slider=3):\n",
    "    \"\"\"\n",
    "    Two models having a conversation back and forth.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The initial thought-provoking prompt\n",
    "        model1: Name of first model\n",
    "        tone1: Tone/personality of first model\n",
    "        model2: Name of second model\n",
    "        tone2: Tone/personality of second model\n",
    "        counter_slider: Number of exchanges between models\n",
    "    \n",
    "    Returns:\n",
    "        A markdown-formatted string of the conversation\n",
    "    \"\"\"\n",
    "    print('Inside talking_models function!!')\n",
    "    print(f\"Model 1: {model1}, Tone: {tone1}\")\n",
    "    print(f\"Model 2: {model2}, Tone: {tone2}\")\n",
    "\n",
    "    # Model mapping\n",
    "    model_map = {\n",
    "        'LLama3.2': llama_model,\n",
    "        'Mistral': mistral_model,\n",
    "        'TinyLlama': tinyllama_model,\n",
    "        'Phi': phi_model\n",
    "    }\n",
    "    \n",
    "    # Tone mapping\n",
    "    tone_map = {\n",
    "        'Rational': rational_tone,\n",
    "        'Philosopher': philosopher_tone,\n",
    "        'Cynic': cynic_tone,\n",
    "        'Adversary': adversary_tone\n",
    "    }\n",
    "    \n",
    "    sys_model1 = model_map[model1]\n",
    "    sys_model2 = model_map[model2]\n",
    "    sys_tone1 = tone_map[tone1]\n",
    "    sys_tone2 = tone_map[tone2]\n",
    "\n",
    "    output = \"\"\n",
    "    conversation = prompt\n",
    "\n",
    "    print(\"Starting Exchanges between models...\")\n",
    "\n",
    "    for i in range(counter_slider):\n",
    "\n",
    "        print(f\"\\n--- Exchange {i+1} ---\")\n",
    "        # Model 1 response\n",
    "        output += f\"**{model1} ({tone1}):** Waiting for response...\\n\\n\"\n",
    "        model1_response = call_model(conversation, sys_model1, sys_tone1)\n",
    "        output = output.replace(f\"**{model1} ({tone1}):** Waiting for response...\\n\\n\", f\"**{model1} ({tone1}):** {model1_response}\\n\\n\")\n",
    "        conversation += \"\\n\\n\" + model1_response\n",
    "        yield output\n",
    "\n",
    "        # Model 2 response\n",
    "        output += f\"**{model2} ({tone2}):** Waiting for response...\\n\\n\"\n",
    "        model2_response = call_model(conversation, sys_model2, sys_tone2)\n",
    "        output = output.replace(f\"**{model2} ({tone2}):** Waiting for response...\\n\\n\", f\"**{model2} ({tone2}):** {model2_response}\\n\\n\")\n",
    "        conversation += \"\\n\\n\" + model2_response\n",
    "        yield output\n",
    "    \n",
    "    print(\"\\nConversation completed successfully!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e9eefb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside talking_models function!!\n",
      "Model 1: LLama3.2, Tone: Philosopher\n",
      "Model 2: TinyLlama, Tone: Cynic\n",
      "Starting Exchanges between models...\n",
      "\n",
      "--- Exchange 1 ---\n",
      "Inside call_model function for llama3.2:1b\n",
      "Inside call_model function for tinyllama\n",
      "\n",
      "--- Exchange 2 ---\n",
      "Inside call_model function for llama3.2:1b\n",
      "Inside call_model function for tinyllama\n",
      "\n",
      "--- Exchange 3 ---\n",
      "Inside call_model function for llama3.2:1b\n",
      "Inside call_model function for tinyllama\n",
      "Conversation completed successfully!!\n"
     ]
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "import gradio as gr\n",
    "\n",
    "msg_input = gr.Textbox(\n",
    "    label='Your Message', \n",
    "    info='Give a thought provoking prompt', \n",
    "    lines=7\n",
    ")\n",
    "\n",
    "model1_selector = gr.Dropdown(\n",
    "    ['LLama3.2', 'Mistral', 'TinyLlama', 'Phi'], \n",
    "    label='Select Model-1', \n",
    "    value='LLama3.2'\n",
    ")\n",
    "\n",
    "model1_tone = gr.Dropdown(\n",
    "    ['Rational', 'Philosopher', 'Cynic', 'Adversary'], \n",
    "    label='Tone of Model-1', \n",
    "    value='Rational'\n",
    ")\n",
    "\n",
    "model2_selector = gr.Dropdown(\n",
    "    ['LLama3.2', 'Mistral', 'TinyLlama', 'Phi'], \n",
    "    label='Select Model-2', \n",
    "    value='Mistral'\n",
    ")\n",
    "\n",
    "model2_tone = gr.Dropdown(\n",
    "    ['Rational', 'Philosopher', 'Cynic', 'Adversary'], \n",
    "    label='Tone of Model-2', \n",
    "    value='Philosopher'\n",
    ")\n",
    "\n",
    "counter_slider = gr.Slider(\n",
    "    1, 5, \n",
    "    value=3, \n",
    "    step=1, \n",
    "    label='Number of Exchanges'\n",
    ")\n",
    "\n",
    "model_output = gr.Markdown(label=\"Response\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=talking_models,\n",
    "    title='2-Model Thought Elaborator',\n",
    "    inputs=[msg_input, model1_selector, model1_tone, model2_selector, model2_tone, counter_slider],\n",
    "    outputs=[model_output],\n",
    "    examples=[\n",
    "        [\n",
    "            \"If gravity were even slightly stronger or weaker, the universe would collapse into chaos or drift into lifeless emptiness, and such delicate precision whispers not of accident, but of intentional design.\", \n",
    "            \"Mistral\", \n",
    "            \"Adversary\", \n",
    "            \"LLama3.2\", \n",
    "            \"Philosopher\",\n",
    "            5\n",
    "        ],\n",
    "        [\n",
    "            \"Ever tried, ever failed, no matter, try again, fail again, fail better!\", \n",
    "            \"LLama3.2\", \n",
    "            \"Philosopher\", \n",
    "            \"TinyLlama\", \n",
    "            \"Cynic\",\n",
    "            5\n",
    "        ],\n",
    "        [\n",
    "            \"You have to be a fighter, because if you don't fight for your love, what kind of love do you have\", \n",
    "            \"TinyLlama\", \n",
    "            \"Cynic\", \n",
    "            \"Phi\", \n",
    "            \"Rational\",\n",
    "            5\n",
    "        ],\n",
    "        [\n",
    "            \"The only true wisdom is in knowing you know nothing.\", \n",
    "            \"Phi\", \n",
    "            \"Philosopher\", \n",
    "            \"LLama3.2\", \n",
    "            \"Adversary\",\n",
    "            5\n",
    "        ]\n",
    "    ],\n",
    "    flagging_mode='never'\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0704e7",
   "metadata": {},
   "source": [
    "Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a974070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model_stream(prompt, model_name, tone_system):\n",
    "    \"\"\"\n",
    "    Call the LLM and stream the response character by character.\n",
    "    \"\"\"\n",
    "    print(f\"Inside call_model_stream function for {model_name}\")\n",
    "\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': tone_system},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_name, \n",
    "        messages=messages,\n",
    "        stream=True  # ADD THIS\n",
    "    )\n",
    "    \n",
    "    # Stream and yield each chunk\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            yield chunk.choices[0].delta.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2a4a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_models(prompt, model1, tone1, model2, tone2, counter_slider=5):\n",
    "    \"\"\"\n",
    "    Two models having a conversation back and forth with true streaming.\n",
    "    \"\"\"\n",
    "    print('Inside streaming_models function!!')\n",
    "    print(f\"Model 1: {model1}, Tone: {tone1}\")\n",
    "    print(f\"Model 2: {model2}, Tone: {tone2}\")\n",
    "\n",
    "    # Model mapping\n",
    "    model_map = {\n",
    "        'LLama3.2': llama_model,\n",
    "        'Mistral': mistral_model,\n",
    "        'TinyLlama': tinyllama_model,\n",
    "        'Phi': phi_model\n",
    "    }\n",
    "    \n",
    "    # Tone mapping\n",
    "    tone_map = {\n",
    "        'Rational': rational_tone,\n",
    "        'Philosopher': philosopher_tone,\n",
    "        'Cynic': cynic_tone,\n",
    "        'Adversary': adversary_tone\n",
    "    }\n",
    "    \n",
    "    sys_model1 = model_map[model1]\n",
    "    sys_model2 = model_map[model2]\n",
    "    sys_tone1 = tone_map[tone1]\n",
    "    sys_tone2 = tone_map[tone2]\n",
    "\n",
    "    output = \"\"\n",
    "    conversation = prompt\n",
    "\n",
    "    print(\"\\nStarting Exchanges between models...\")\n",
    "\n",
    "    for i in range(counter_slider):\n",
    "\n",
    "        print(f\"\\n--- Exchange {i+1} ---\")\n",
    "\n",
    "        # Model 1 response\n",
    "        output += f\"**{model1} ({tone1}):** \"\n",
    "        yield output\n",
    "        \n",
    "        model1_response = \"\"\n",
    "        for chunk in call_model_stream(conversation, sys_model1, sys_tone1):\n",
    "            model1_response += chunk\n",
    "            output += chunk\n",
    "            yield output\n",
    "        \n",
    "        output += \"\\n\\n\"\n",
    "        conversation += \"\\n\\n\" + model1_response\n",
    "        yield output\n",
    "\n",
    "        # Model 2 response\n",
    "        output += f\"**{model2} ({tone2}):** \"\n",
    "        yield output\n",
    "        \n",
    "        model2_response = \"\"\n",
    "        for chunk in call_model_stream(conversation, sys_model2, sys_tone2):\n",
    "            model2_response += chunk\n",
    "            output += chunk\n",
    "            yield output\n",
    "        \n",
    "        output += \"\\n\\n\"\n",
    "        conversation += \"\\n\\n\" + model2_response\n",
    "        yield output\n",
    "    \n",
    "    print(\"\\nConversation completed successfully!!\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0857bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside streaming_models function!!\n",
      "Model 1: LLama3.2, Tone: Philosopher\n",
      "Model 2: TinyLlama, Tone: Cynic\n",
      "\n",
      "Starting Exchanges between models...\n",
      "\n",
      "--- Exchange 1 ---\n",
      "Inside call_model_stream function for llama3.2:1b\n",
      "Inside call_model_stream function for tinyllama\n",
      "\n",
      "--- Exchange 2 ---\n",
      "Inside call_model_stream function for llama3.2:1b\n",
      "Inside call_model_stream function for tinyllama\n",
      "\n",
      "--- Exchange 3 ---\n",
      "Inside call_model_stream function for llama3.2:1b\n",
      "Inside call_model_stream function for tinyllama\n",
      "\n",
      "Conversation completed successfully!!\n"
     ]
    }
   ],
   "source": [
    "# Gradio Interface\n",
    "import gradio as gr\n",
    "\n",
    "msg_input = gr.Textbox(\n",
    "    label='Your Message', \n",
    "    info='Give a thought provoking prompt', \n",
    "    lines=7\n",
    ")\n",
    "\n",
    "model1_selector = gr.Dropdown(\n",
    "    ['LLama3.2', 'Mistral', 'TinyLlama', 'Phi'], \n",
    "    label='Select Model-1', \n",
    "    value='LLama3.2'\n",
    ")\n",
    "\n",
    "model1_tone = gr.Dropdown(\n",
    "    ['Rational', 'Philosopher', 'Cynic', 'Adversary'], \n",
    "    label='Tone of Model-1', \n",
    "    value='Rational'\n",
    ")\n",
    "\n",
    "model2_selector = gr.Dropdown(\n",
    "    ['LLama3.2', 'Mistral', 'TinyLlama', 'Phi'], \n",
    "    label='Select Model-2', \n",
    "    value='Mistral'\n",
    ")\n",
    "\n",
    "model2_tone = gr.Dropdown(\n",
    "    ['Rational', 'Philosopher', 'Cynic', 'Adversary'], \n",
    "    label='Tone of Model-2', \n",
    "    value='Philosopher'\n",
    ")\n",
    "\n",
    "counter_slider = gr.Slider(\n",
    "    1, 5, \n",
    "    value=3, \n",
    "    step=1, \n",
    "    label='Number of Exchanges'\n",
    ")\n",
    "\n",
    "model_output = gr.Markdown(label=\"Response\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=streaming_models,\n",
    "    title='2-Model Thought Elaborator',\n",
    "    inputs=[msg_input, model1_selector, model1_tone, model2_selector, model2_tone, counter_slider],\n",
    "    outputs=[model_output],\n",
    "    examples=[\n",
    "        [\n",
    "            \"If gravity were even slightly stronger or weaker, the universe would collapse into chaos or drift into lifeless emptiness, and such delicate precision whispers not of accident, but of intentional design.\", \n",
    "            \"Mistral\", \n",
    "            \"Adversary\", \n",
    "            \"LLama3.2\", \n",
    "            \"Philosopher\",\n",
    "            3\n",
    "        ],\n",
    "        [\n",
    "            \"Ever tried, ever failed, no matter, try again, fail again, fail better!\", \n",
    "            \"LLama3.2\", \n",
    "            \"Philosopher\", \n",
    "            \"TinyLlama\", \n",
    "            \"Cynic\",\n",
    "            3\n",
    "        ],\n",
    "        [\n",
    "            \"You have to be a fighter, because if you don't fight for your love, what kind of love do you have\", \n",
    "            \"TinyLlama\", \n",
    "            \"Cynic\", \n",
    "            \"Phi\", \n",
    "            \"Rational\",\n",
    "            3\n",
    "        ],\n",
    "        [\n",
    "            \"The only true wisdom is in knowing you know nothing.\", \n",
    "            \"Phi\", \n",
    "            \"Philosopher\", \n",
    "            \"LLama3.2\", \n",
    "            \"Adversary\",\n",
    "            3\n",
    "        ]\n",
    "    ],\n",
    "    flagging_mode='never'\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
